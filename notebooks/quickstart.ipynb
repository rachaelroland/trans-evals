{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trans-evals Quick Start Tutorial\n",
    "\n",
    "This notebook demonstrates basic usage of the trans-evals framework for evaluating LLM bias toward trans and non-binary individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nsys.path.append('..')  # Add parent directory to path\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Import trans-evals modules\nfrom src import (\n    load_dataset,\n    BiasEvaluator,\n    TransSpecificTemplates,\n    OpenRouterModel\n)\nfrom src.augmentation import CounterfactualAugmenter\nfrom src.datasets.base import BiasType"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Datasets\n",
    "\n",
    "Let's explore the available datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TANGO dataset\n",
    "tango = load_dataset(\"tango\")\n",
    "print(f\"TANGO dataset has {len(tango)} examples\")\n",
    "\n",
    "# Look at a few examples\n",
    "for i, example in enumerate(tango[:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Text: {example.text}\")\n",
    "    print(f\"  Bias Type: {example.bias_type.value}\")\n",
    "    print(f\"  Target Identity: {example.target_identity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HolisticBias with filtering\n",
    "holistic = load_dataset(\n",
    "    \"holistic_bias\",\n",
    "    identity_terms=[\"trans woman\", \"trans man\", \"non-binary\"]\n",
    ")\n",
    "print(f\"Filtered HolisticBias has {len(holistic)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Template-Based Test Generation\n",
    "\n",
    "Generate custom test cases using templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pronoun consistency templates\n",
    "pronoun_templates = TransSpecificTemplates.get_pronoun_templates()\n",
    "print(f\"Number of pronoun templates: {len(pronoun_templates)}\")\n",
    "\n",
    "# Generate examples from first template\n",
    "first_template = pronoun_templates[0]\n",
    "examples = first_template.generate_examples()\n",
    "print(f\"\\nGenerated {len(examples)} examples from template\")\n",
    "\n",
    "# Show a few\n",
    "for ex in examples[:3]:\n",
    "    print(f\"\\n- {ex.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Counterfactual Data Augmentation\n",
    "\n",
    "Create counterfactual examples by swapping identity terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize augmenter\n",
    "augmenter = CounterfactualAugmenter()\n",
    "\n",
    "# Create a test example\n",
    "from src.datasets.base import DatasetExample, EvaluationType\n",
    "\n",
    "original = DatasetExample(\n",
    "    id=\"test_1\",\n",
    "    text=\"The trans woman presented her research at the conference.\",\n",
    "    bias_type=BiasType.SENTIMENT,\n",
    "    evaluation_type=EvaluationType.GENERATION,\n",
    "    target_identity=\"trans woman\"\n",
    ")\n",
    "\n",
    "# Create counterfactual\n",
    "counterfactual = augmenter.create_counterfactual(original)\n",
    "\n",
    "print(\"Original:\", original.text)\n",
    "print(\"Counterfactual:\", counterfactual.text)\n",
    "print(\"Replacements:\", counterfactual.metadata['replacements'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Bias Evaluation\n",
    "\n",
    "Evaluate a model on our test cases.\n",
    "\n",
    "**Note**: This requires setting up API keys in your .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if API key is available\nif os.getenv(\"OPENROUTER_API_KEY\"):\n    # Initialize model with OpenRouter\n    model = OpenRouterModel(model_name=\"gpt-3.5-turbo\")\n    print(f\"Using model: {model.model_name}\")\n    \n    # Create small test dataset\n    test_examples = tango[:3]  # Just 3 examples for demo\n    \n    # Initialize evaluator\n    evaluator = BiasEvaluator(model=model)\n    \n    # Run evaluation\n    results = evaluator.evaluate(\n        test_examples,\n        metrics_to_use=[\"toxicity\", \"sentiment\"]\n    )\n    \n    # Show results\n    print(\"\\nEvaluation Results:\")\n    print(f\"Examples evaluated: {len(results.examples)}\")\n    \n    # Show individual results\n    for i, (ex, pred, metrics) in enumerate(zip(results.examples, results.predictions, results.metric_results)):\n        print(f\"\\nExample {i+1}:\")\n        print(f\"  Input: {ex.text[:50]}...\")\n        print(f\"  Output: {pred[:50]}...\")\n        for metric_name, metric_result in metrics.items():\n            print(f\"  {metric_name}: {metric_result.score:.3f}\")\nelse:\n    print(\"Please set OPENROUTER_API_KEY in your .env file to run evaluation\")\n    print(\"Get your API key from: https://openrouter.ai/keys\")\n    print(\"\\nYou can test with various models:\")\n    for name in [\"gpt-3.5-turbo\", \"claude-3-haiku\", \"mistral-7b\", \"llama-2-13b\"]:\n        print(f\"  - {name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Results\n",
    "\n",
    "Convert results to DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock results for demonstration\n",
    "from src.evaluation.evaluator import EvaluationResult\n",
    "from src.evaluation.metrics import MetricResult\n",
    "from datetime import datetime\n",
    "\n",
    "# Create sample results\n",
    "mock_results = EvaluationResult(\n",
    "    dataset_name=\"demo\",\n",
    "    model_name=\"demo-model\",\n",
    "    timestamp=datetime.now().isoformat()\n",
    ")\n",
    "\n",
    "# Add some examples\n",
    "for i in range(5):\n",
    "    ex = DatasetExample(\n",
    "        id=f\"ex_{i}\",\n",
    "        text=f\"Test example {i}\",\n",
    "        bias_type=BiasType.TOXICITY,\n",
    "        evaluation_type=EvaluationType.GENERATION,\n",
    "        target_identity=\"trans woman\" if i % 2 == 0 else \"trans man\"\n",
    "    )\n",
    "    mock_results.examples.append(ex)\n",
    "    mock_results.predictions.append(f\"Generated text {i}\")\n",
    "    mock_results.metric_results.append({\n",
    "        \"toxicity\": MetricResult(\"toxicity\", 0.1 + i * 0.05),\n",
    "        \"sentiment\": MetricResult(\"sentiment\", 0.7 - i * 0.1)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = mock_results.to_dataframe()\n",
    "print(\"Results DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Analyze by identity\n",
    "print(\"\\nAverage scores by identity:\")\n",
    "print(df.groupby('target_identity')[['toxicity_score', 'sentiment_score']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Bias Patterns\n",
    "\n",
    "Create visualizations to understand bias patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create sample data for visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample bias scores\n",
    "identities = ['trans woman', 'trans man', 'non-binary', 'cis woman', 'cis man']\n",
    "metrics = ['toxicity', 'misgendering', 'regard', 'sentiment']\n",
    "\n",
    "data = []\n",
    "for identity in identities:\n",
    "    for metric in metrics:\n",
    "        # Generate realistic-looking scores\n",
    "        if 'trans' in identity and metric == 'toxicity':\n",
    "            score = np.random.normal(0.3, 0.1)\n",
    "        elif 'trans' in identity and metric == 'misgendering':\n",
    "            score = np.random.normal(0.7, 0.15)\n",
    "        else:\n",
    "            score = np.random.normal(0.85, 0.1)\n",
    "        \n",
    "        data.append({\n",
    "            'identity': identity,\n",
    "            'metric': metric,\n",
    "            'score': np.clip(score, 0, 1)\n",
    "        })\n",
    "\n",
    "viz_df = pd.DataFrame(data)\n",
    "\n",
    "# Create heatmap\n",
    "pivot_df = viz_df.pivot(index='identity', columns='metric', values='score')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_df, annot=True, fmt='.2f', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "plt.title('Bias Scores by Identity and Metric\\n(Higher is better except for toxicity)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Expand evaluation**: Test on more examples and datasets\n",
    "2. **Compare models**: Evaluate multiple models to find the least biased\n",
    "3. **Custom metrics**: Implement domain-specific bias metrics\n",
    "4. **Mitigation**: Use results to improve model behavior\n",
    "\n",
    "For more examples and advanced usage, see the `docs/USER_GUIDE.md`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}