https://chatgpt.com/share/68769b9c-b468-8003-88ff-31612de24703

# Datasets for Evaluating LLM Bias Toward Trans Femme Individuals

## Specialized Datasets Centering Trans and Non-Binary Biases

* **TANGO (Trans And Nonbinary Gender-Oriented dataset)** – A recent dataset from 2023 that explicitly focuses on biases affecting transgender and non-binary (TGNB) people. TANGO uses **template-based prompts curated from real TGNB community texts** to evaluate two key issues: (1) **Misgendering** – whether a model uses the correct pronouns/gendered terms for a trans or non-binary subject, and (2) **Harmful responses to gender disclosure** – how a model reacts when a character discloses their trans identity. For example, the authors prompt language models with sentences containing TGNB gender disclosures and measure if outputs become stigmatizing or toxic. Their findings show that *LLMs tend to handle binary pronouns better than singular “they” or neopronouns (misgendering is worst with neutral/neopronouns)*, and that *model generations can turn more toxic when triggered by a trans identity disclosure*. This dataset provides a **well-annotated benchmark grounded in TGNB voices** for evaluating bias (e.g. incorrect pronouns or offensive language) in open-ended text generation. *Use case:* evaluating chatbots or text generators for respectful language (no misgendering) and non-harmful content when talking about or with trans femme individuals.

* **Jigsaw Unintended Bias in Toxicity (Identity Subset)** – While not solely about trans people, this real-world comment dataset (from a Google Jigsaw Kaggle challenge) includes **identity labels for each comment**, including a feature for whether the comment mentions *“transgender”* (as well as other identities). Each comment is also labeled for toxicity. This allows evaluation of **unintended bias**: e.g., checking if a toxicity classifier or LLM disproportionately flags benign comments that simply mention trans identities as “toxic.” The dataset’s **clear annotations** of identity mentions make it useful for testing whether models treat content about trans folks unfairly. *Use case:* measuring false-positive rates of a moderation model on sentences like “She is a transgender woman” versus “She is a woman,” or prompting an LLM with identity-related content to see if it produces or filters output differently. (Researchers often report that models trained on toxic data may mistakenly associate terms like “gay” or “transgender” with toxicity; this dataset helps quantify that bias.)

* **RedditBias (Queerness Category)** – A *real-world corpus* of over 11k Reddit comments used to evaluate bias in conversational AI. Comments are grouped by target demographics – including *gender* and *queerness* – meaning some posts involve LGBTQ+ or trans topics. The dataset has been used to test language models by measuring perplexity or response differences when a prompt references different groups. While not exclusively trans-focused, it provides *naturally occurring examples* of how people talk about trans/non-binary individuals (both positive and negative). It can reveal if a model responds in a biased way (e.g., is a completion less coherent or more offensive when the prompt involves a trans character versus a cis character?). *Use case:* assessing dialogue models for subtle biases on forums – for instance, checking if a chatbot’s next-turn response to a trans-related comment is less helpful or more toxic than to an otherwise similar comment about a cis person.

## General Gender Bias Datasets Adaptable to Trans Femme Evaluation

* **HolisticBias (Facebook AI, 2022)** – A large-scale template dataset containing nearly **600 demographic descriptor terms across 13 axes (e.g. gender, religion, nationality, etc.)**. Notably, the gender axis includes **multiple gender identities** beyond the binary, such as *“trans femme,” “trans masculine,” “transgender,”* etc.. HolisticBias provides **450k templated sentences** (v1.0) generated by inserting these identity terms into various sentence frames. For example, one template might be *“This is a {noun} who is {descriptor}.”* By swapping `{descriptor}` with entries like *“a trans woman”* or *“a cis woman,”* one can measure changes in model behavior. The dataset is open-sourced with a GitHub generation script and is described as a *“living” evaluation set* meant to grow over time. **Use cases:** (1) *Likelihood bias:* feed different identity sentences into a language model and see if perplexity or predicted probabilities differ systematically (indicative of bias). (2) *Generation tests:* use the sentences as prompts and analyze differences in continuations. (3) *Toxicity triggers:* test if certain identities cause the model to flag the content as offensive or to produce unsafe output. HolisticBias’s breadth and inclusion of trans-specific terms make it a strong foundation – simple **fill-in templates with trans femme descriptors** can uncover biased model tendencies in a controlled way.

* **CrowS-Pairs (2020)** – A **crowd-sourced challenge set** of 1,508 English sentence pairs designed to expose stereotyping biases in language models. Each pair consists of a *“stereotypical”* sentence and a matched *“anti-stereotypical”* sentence that differ only in the demographic reference. CrowS-Pairs covers **nine bias categories**, one of which is *gender/gender identity*. For example, a pair might be: “*The **transgender** person was very aggressive*” vs. “*The **cisgender** person was very aggressive*” (hypothetically illustrating a stereotype). A fair model should assign equal (or appropriate) likelihood to both sentences; a biased model might systematically prefer the stereotype. **Use in LLM evaluation:** researchers have prompted language models with each pair to see which sentence the model deems more probable or which it completes more positively. Because CrowS-Pairs explicitly includes *gender identity* biases alongside race, religion, etc., it can be adapted to check biases **against trans femme individuals**. For instance, you can filter or create pairs focusing on trans women (e.g., contrasting “trans woman” vs “woman” in an identical context) and measure if the LLM output favors one in a biased manner. *CrowS-Pairs is available on Hugging Face and GitHub, with annotations identifying the “target” of bias in each pair.*

* **BBQ (Bias Benchmark for Question Answering, 2022)** – A **hand-built QA dataset** with 58k examples targeting biases across **9 social dimensions**. These dimensions include *gender identity, sexual orientation, race/ethnicity, disability,* and more. BBQ is structured into context + question + answer choice sets: each question is crafted in two versions – one under-informative (requiring the model to rely on potential stereotypes) and one disambiguated with factual context. For example, a context might implicitly refer to a person with they/them pronouns or mention someone is transgender, and the question asks something like “Who is a good parent in the story?” with answer choices that could reveal bias. **Why it’s useful:** It evaluates *if a model’s QA behavior is influenced by bias* – e.g., does the model pick a negative answer about a trans person when context is ambiguous? Parrish et al. report that models often **default to harmful stereotypes when context is lacking**, and even with context, they perform worse when the correct answer goes against a bias. BBQ’s detailed annotation of bias type per question means one can isolate the **transgender-related question sets** (under the gender identity or sexual orientation categories). *Use case:* benchmark an LLM’s fairness in a QA setting – ensure it isn’t systematically erring on questions involving trans femme individuals (for instance, always attributing negative traits or outcomes to trans characters). BBQ is open-source (CC BY 4.0) and comes with evaluation scripts to calculate bias metrics (like accuracy gaps between “stereotype-aligned” and “stereotype-opposed” answers).

* **StereoSet (2020)** – A crowdsourced dataset similar in spirit to CrowS-Pairs, with **16k multiple-choice questions** measuring **stereotypical bias** in LMs. It spans four domains: gender, profession, race, religion. In the *gender* category StereoSet mostly addresses binary gender stereotypes (e.g., “men are engineers, women are not”). While it doesn’t explicitly include trans identities, the **methodology can be extended**: StereoSet provides a framework where a prompt is given and the model must choose between a stereotypical continuation, an anti-stereotypical continuation, or a unrelated one. Researchers interested in trans femme bias could create *custom StereoSet-style questions* – for example, prompt: “Alexis is a trans woman who loves science. As a child, Alexis wanted to be a \_\_\_.” and options that reflect a stereotype or a neutral choice. StereoSet’s original data and evaluation code (which computes a “stereotype score” for a model) can thus be adapted to test **trans-specific stereotypes** by adding such examples. *In summary:* StereoSet is a general bias benchmark for *stereotype completion* tasks; it can be a template for evaluating whether an LLM’s continuations about trans femme people default to clichés or not. (The dataset is available on Hugging Face and includes a leaderboard, making it a well-documented resource, though direct trans content must be user-augmented.)

* **WinoBias/WinoGender Schemas** – These are **coreference resolution** test sets originally created to expose gender bias in resolving pronouns. In WinoGender (2018), sentences like “*The nurse handed the doctor his clipboard…*” are given, and the task is to identify if “his” refers to the nurse or doctor; models often erred due to gender stereotypes (assuming male doctor, female nurse). While originally binary, *some recent variants include non-binary pronouns* (e.g., a Swedish Winogender dataset explicitly has a *ternary gender setting with hen (neutral pronoun))*. To leverage this for trans femme bias, one could construct **Winogender-style sentences with trans identities** – e.g., “*The trans woman mentor advised the young student to follow her dreams, but **she** struggled to find a role model.*” Does the model understand that “she” refers to the trans woman mentor, or does it confuse references because of implicit bias? This kind of evaluation checks if the model’s internal coreference or reasoning is confounded by a person being trans. *Use case:* After fine-tuning an LLM for better coreference, include some pronoun test cases with trans characters (possibly using neo-pronouns or mixing binary and neutral terms) to ensure it doesn’t default to wrong genders. (These tailored examples would be **simple and targeted**, and could be generated similarly to WinoBias templates.) For existing data, the **Winogender** and **WinoBias** sets (available via the **Hugging Face datasets** library) can serve as a starting point – they come with answer keys and are often used in fairness research for measuring gender bias in model predictions.

## Synthetic Bias Data Generation Methods and Papers

* **Template-Based Counterfactual Data Augmentation:** A common strategy to create bias evaluation data is to **generate sentence pairs or groups that differ only in the demographic descriptor**, then compare model outputs. Several papers advocate this:

  * *Smith et al. (2022)* with HolisticBias used **template sentences** filled with various identity terms. This approach can be extended by researchers to new identities easily – e.g., write a template sentence like “*I am a \_\_\_ looking for a job*” and fill the blank with \[`woman`, `man`, `trans woman`, `non-binary person`], etc., to probe model differences.
  * *Qian et al. (2022)* (“Perturbation Augmentation for Fairer NLP”) introduced the **PANDA** dataset, which consists of \~100k **perturbed sentence pairs**. They took diverse sentences and algorithmically **rewrote them to flip the demographic attributes** (gender, race, or age), while keeping meaning constant. For example, “*The **waiter** served his customer*” could be perturbed to “*The **waitress** served her customer*.” Although their focus was on mitigation (training models on these perturbations), the same data serves as a synthetic evaluation set: one can check if a model’s answer or behavior changes when “waiter” is replaced with “waitress” or “server.” This method could be applied to *trans femme vs cis femme* references (e.g., swap “woman” ↔ “trans woman”) to **generate evaluation data systematically**. The PANDA code and data (when released) provide a blueprint for such controlled data generation.
  * *Counterfactual Prompting:* Even without a pre-built dataset, one can use **find-and-replace on existing corpora** or prompts to create counterfactual versions. For instance, take a news article sentence about a person and insert “transgender” before the word “woman,” and see if a summarization model’s output changes. This technique is recommended in guides for bias testing – e.g., using **counterfactual augmentation to flip attributes like gender or race** in prompts and observing output differences. It’s a simple yet powerful way to *generate tailored bias test cases on the fly*.

* **Synthetic Question/Dialogue Generation:** Some research proposes using language models themselves to **generate bias-testing data**. For example, the **HolisticBias** team mentions their dataset is extensible and invites new terms. Likewise, one could prompt a model with instructions to produce scenarios: *“Write a short dialogue where a user comes out as a trans woman, and the assistant responds.”* By controlling or annotating the assistant’s response (whether it’s respectful or contains bias), you create synthetic QA or conversational items for evaluation. There are also bias *benchmark pipelines* like **SAGED** or **FairBench** that allow plugging in custom identity lists to produce test cases (these are emerging libraries that integrate generation + evaluation, often configurable via YAML or code). While not a specific dataset, these methods are described in fairness literature as ways to *programmatically create new bias datasets* that are simple and extensible. *For a concrete example:* the **UNQOVER** framework (Li et al. 2020) generated *underspecified questions* to reveal biases – one could adapt their templates to include trans-related content, producing Q\&A pairs where only biased reasoning would lead the model to choose a particular answer.

* **Publications proposing synthetic bias tests:** A few papers stand out for methodology:

  * *“I’m Sorry to Hear That”: Finding Biases with a Holistic Descriptor Dataset* (Smith et al. EMNLP 2022) – introduces HolisticBias and details the template generation and bias metrics (likelihood bias, generation bias, etc.).
  * *“Bias in Open-Ended Generation (BOLD)”* (Dhamala et al. FAccT 2021) – while BOLD itself compiled prompts from Wikipedia, the authors also proposed **bias metrics** for generation (comparing sentiment or toxicity of completions across groups). They show how to measure, say, if generations about one demographic are consistently more negative. This can inspire synthetic test design: for trans femme evaluation, you might compute an average “regard” score for completions of prompts about trans women versus about cis women.
  * *“HONEST – Measuring Hurtful Sentence Completion”* (Nozza et al. NAACL 2021) – they crafted prompts in multiple languages and measured how often a language model’s completion was *hurtful or derogatory* toward the described identity. HONEST focused on binary gender (28 terms like “woman, girl, man, boy” etc.), but the technique could be extended: include *“trans woman”* in the identity list and use their templates (e.g., “The \_\_ person is”) to see if the model generates slurs or insults more for some identities than others. The paper provides a methodology for using a hate speech lexicon or classifier to flag completions as hurtful. This is essentially *synthetic prompt generation plus automated evaluation*, which could be replicated for trans femme identities by updating the identity lexicon.

In summary, synthetic data generation for bias eval typically uses **controlled templates or perturbations**. These methods ensure that any difference in model output can be attributed to the identity term change. They produce *simple, general-purpose test cases* (ideal for a foundation that can later be expanded). Combining several approaches – e.g., *templates for stereotypes, counterfactual swaps in real sentences, and model-generated scenarios with trans characters* – often gives a more complete picture of biases.

## Tools and Libraries for Building or Extending Bias Datasets

* **CheckList (Behavioral Testing Toolkit)** – CheckList is an open-source library that makes it easy to generate and organize test cases for NLP models. It provides a simple template language and a lexicon of terms so you can create many sentence variants with minimal effort. For example, you can define a template `"I am a {}."` and supply a list `["woman", "trans woman", "nonbinary person"]` to automatically produce test sentences. CheckList also lets you define expected model behaviors for each test (pass/fail conditions) and aggregate results. This tool was designed to *cover different linguistic capabilities and fairness checks* systematically. In the context of trans femme bias: you could use CheckList to create a suite of tests (templates for misgendering, for stereotypes, for toxic responses) and then run an LLM through them to see where it fails. It’s very practical for spinning up **custom mini-datasets** without hand-writing every example. *(The CheckList GitHub includes examples of gender swap tests, and it was used in a user study to find twice as many bugs related to bias and robustness.)*

* **Hugging Face Datasets and Evaluate Libraries** – Hugging Face hosts many of the mentioned datasets (HolisticBias, CrowS-Pairs, BBQ, etc.) and provides a unified API to load them in Python. This means you can quickly load, say, `load_dataset("crows_pairs")` or `load_dataset("fairnlp/holistic-bias", "sentences")` to get the data, then subset or modify it for trans-related evaluation. The **Evaluate** library offers pre-built metrics that are very useful for bias analysis. For instance, the `evaluate.load("toxicity")` metric uses a hate-speech classifier to score text for toxicity. You can generate model outputs for prompts about trans individuals and measure toxicity or **“regard”** (a metric from Sheng et al. that scores how respectful/positive a completion is toward a target group). Hugging Face’s blog (Luccioni *et al.*, 2022) demonstrates such workflows: they show that simply changing a prompt’s pronoun from “he” to “she” led to a higher toxic content score in GPT-2’s continuation – a clear sign of bias. The Evaluate library also includes **polarity metrics** (to detect sentiment bias) and allows custom evaluation functions. Overall, these tools enable *rapid experimentation*: you can mix and match existing datasets and new prompts, then apply standardized metrics to quantify bias.

* **Perspective API and Other Toxicity Classifiers:** Google’s Perspective API (and similar models like OpenAI’s content filter or the **RADI** model used in HF’s toxicity metric) can be leveraged to analyze model outputs for *hate or harassment specifically toward trans folks*. If you generate text completions or dialogues about a trans woman, running them through such classifiers can flag if the language became demeaning or violent. This helps **automate the scoring of bias** in generated text, especially for large-scale eval sets. Some research benchmarks define their own metrics using these tools – e.g., the *“regard score”* mentioned above (Sheng et al. 2019) classified GPT-2 outputs about different identities as positive, neutral, or negative regard. They found certain groups (like sexual minorities) got more negative completions. As a practitioner, you can use these pretrained classifiers or libraries like **HateXplain** to annotate your model’s outputs for bias evaluation. *One caution:* Ensure the classifier itself is accurate for LGBTQ+ content (some off-the-shelf toxicity detectors had unintended bias, which is ironically what we’re testing!). Still, they are practical for measuring *relative* differences in how an LLM treats different identity inputs.

* **Bias Evaluation Suites/Benchmarks:** There are integrated frameworks, often from academia, that bundle datasets and metrics together. For example, the **LM Bias Evaluation Harness** (by EleutherAI) includes CrowS-Pairs, StereoSet, etc., and can compute perplexity-based bias scores for language models out-of-the-box. Similarly, the **Holistic AI** open-source tool (not to be confused with the HolisticBias data) aims to provide a pipeline to *measure and mitigate bias* – it supports various bias metrics and can be customized with your own data. While these are more technical, they save time if you want to run a broad suite of bias tests (including for gender identity bias) on a new model. They often come with default visualizations and statistical significance checks. For a lightweight approach, even a simple **Colab notebook** with the Hugging Face libraries can serve as your “tool”: load a dataset, generate outputs from your model, and calculate bias metrics. The key is that a lot of the heavy lifting (data collection and metric implementation) is already done by the community and available through these libraries.

In conclusion, there are several **well-documented datasets** and **resources** to get started on evaluating LLM bias toward trans femme individuals. A good initial strategy is to combine a **broad benchmark** (like HolisticBias or CrowS-Pairs for general bias trends) with a **targeted dataset** centering trans experiences (like TANGO for misgendering and toxicity in context). This ensures coverage of both *linguistic biases* (e.g. pronoun misuse, name erasure) and *social biases* (stereotypes, hate speech). From there, **custom augmentation** using tools like CheckList or counterfactual swaps can fill any gaps with simple examples tailored to trans femme representation. All the mentioned datasets come with documentation and have been cited in recent fairness research, so they provide a solid, credible foundation for iterative improvement in bias evaluation. By using these resources and tools in combination, researchers and developers can begin to quantify and address how LLMs might behave differently for trans femme inputs, leading to more inclusive and fair language technologies.

**Sources:**

* Smith et al. (2022), *“I'm Sorry to Hear That”: Finding New Biases in Language Models with a Holistic Descriptor Dataset* – introduction of **HolisticBias** (600 identity terms, templates).
* Ovalle et al. (2023), *“I’m fully who I am”: Centering Transgender and Non-Binary Voices...* – introduction of **TANGO** dataset, misgendering and harm findings.
* Parrish et al. (2022), *“BBQ: A Hand-Built Bias Benchmark for Question Answering”* – **BBQ** dataset with bias in QA (9 categories incl. gender identity).
* Nangia et al. (2020), *“CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked LMs”* – **CrowS-Pairs** (paired stereotypes, includes gender identity bias).
* Nozza et al. (2021), *“HONEST: Measuring Hurtful Sentence Completion in LMs”* – methodology for hurtful completions (identity templates, classifier).
* Dhamala et al. (2021), *“BOLD: Bias in Open-Ended Language Generation”* – **BOLD** dataset (Wikipedia prompts, bias metrics).
* Jigsaw/Conversation AI, *“Unintended Bias in Toxicity Classification”* – identity-labeled toxic comments (transgender tag).
* Luccioni et al. (2022), Hugging Face Blog *“Evaluating LLM Bias with 🤗 Evaluate”* – examples of using **Winogender/WinoBias** prompts and toxicity metrics to reveal bias.
* Ribeiro et al. (2020), *“Beyond Accuracy: Behavioral Testing with CheckList”* – CheckList tool for template-based test generation.
* Additional references and source links: HolisticBias on HuggingFace, CrowS-Pairs on HF, SafetyPrompts repository (bias dataset index).
